---
title: "STAT 172 Final Deliverable"
author: "Ethan, Jack, & Nick"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)

#installing packages and getting library

# install.packages("dplyr")
# install.packages("tidyr")
# install.packages("lubridate")
#install.packages("devtools")
# devtools::install_github("edwinth/paletti") 
#devtools::install_github("thomasp85/patchwork")
#install.packages("MASS")


library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(devtools)
library(paletti)
library(patchwork)
library(RColorBrewer)
library(randomForest)
library(pROC)
library(tidytext)
library(reshape2)
library(glmnet)
library(MASS)

```

# Predicting whether or not a goal is scored for the Boston Bruins 

The goal of this analysis is to evaluate the historical performance of Boston Bruins skaters. 

On any team it is essential for players to know their role. An example of this are goal scorers, who are expected by coaches and teammates to have consistent offensive performance and propel the team to more victories. Thus, this analysis will look at every player from the Boston Bruins to identify and predict whether or not a player will score one or more goals during a game. 

If the analysis accurately predicts a player scoring a goal, this will be reflective of the player doing their role. However, if the algorithm predicts that a player should have scored, and doesn't, this provides an opportunity for coaches to reevaluate game film and make adjustments to the player's practice regimen. 

Ultimately, this will enhance the film review process for coaches, which can be one of their most time consuming processes in game preparation. This algorithm would significantly reduce this time and allow the staff to be more efficient in practice.

## Initial Data Exploration
The data we used in this assignment was pulled from KAGGLE and utilizes two files: Skater_stats and Player_info. Each row in Skater_stats corresponds to one game for one player. Thus, players had games and games had multiple players. We then merged in player_info to allow for more explanatory variables.

The following output is the summary and structure of the data.

```{r data set up, echo=FALSE}
mycols <- c(
  black    = "#010101",
  white = "#FFFFFF",
  gold   = "#FFB81C",
  orange = "#ffd289",
  beige = "#DDCBa4",
  brown = "#744F28"
)


# use 'PLAYER DATA' zip file
# game_skater_stats.csv
skater_stats <-read.csv(file.choose(), header = T)
# player_info.csv
player_info <-read.csv(file.choose(), header = T)

#adjusting data sets to get desired columns
skater_stats <- subset(skater_stats, select = -c(evenTimeOnIce, shortHandedTimeOnIce, powerPlayTimeOnIce))
player_info <- subset(player_info, select = c(player_id, firstName, lastName, nationality, primaryPosition, birthDate, height_cm, weight, shootsCatches))

#merging datasets
skater_stats <- left_join(skater_stats, player_info, by="player_id")

#reducing number of rows in data set by only selecting Boston Bruins Players
skater_stats <- filter(skater_stats, team_id == 6)

#removing extra columns
data <- subset(skater_stats, select = -c(game_id, player_id, team_id))

#fixing the birthDate column to age
data$birthDate <- substr(data$birthDate, 1,10)
data$birthDate <- as_date(data$birthDate)
data$age <- (2022 - year(data$birthDate))

#subsetting data to get final organized data set
data <- subset(data, select = -c(birthDate, plusMinus, powerPlayGoals, shortHandedGoals))
data$score <- ifelse(data$goals >= 1, "Yes","No")


#have missing data in hits, takeaways, giveaways, and blocked, imputing median / more occuring value for all of them
data$hits[is.na(data$hits)] <-median(data$hits[!is.na(data$hits)])
data$takeaways[is.na(data$takeaways)] <-median(data$takeaways[!is.na(data$takeaways)])
data$giveaways[is.na(data$giveaways)] <-median(data$giveaways[!is.na(data$giveaways)])
data$blocked[is.na(data$blocked)] <-median(data$blocked[!is.na(data$blocked)])
data$shootsCatches[is.na(data$shootsCatches)] <- "L"

summary(data)
str(data)

```

### Exploratory Analysis: Visuals

The following visual is box and whisker plot showing the number of shots per game by position.

```{r graphing shots box and whisker plot, echo=FALSE}

# Explanatory Graph with Shots

shots <- ggplot() +
  geom_boxplot(aes(x = primaryPosition, y = shots), data = data, color = "#010101", fill = "#FFB81C")+
  labs(x="Position", y="Shots per game") +
  ggtitle("Shots per Game by Position")

shots


```

From the visual we can see that for Centers (C), Left Wings (LW), and Right Wings (RW) take more shots than Defensemen. When examining the medians we see that C and RW have higher medians, thus we would expect Centers and Right Wings to be scoring more goals since they are taking more shots compared to Defensemen and Left Wings.

Next, let's look at the average goals per game by position:

```{r graphing average goals bar chart, echo=FALSE}

# Explanatory Graph with Goals
avg_by_position = data %>% 
  group_by(primaryPosition) %>%
  summarise(avg_goal = mean(goals)) %>% ungroup


goals <- ggplot() +
  geom_col(aes(x = primaryPosition, y = avg_goal), data = avg_by_position, color = "#010101", fill = "#FFB81C")+
  labs(x="Position", y="Average Goals Scored") +
  ggtitle("Goals per Game by Position")


goals


```

This chart confirms out hypoethesis from the box plot. We can see that Centers and Right Wings score more goals on average than Defensemen and Left wings. In addition, Right Wings will score the most amount of goals per game on average. It is logical that Defensemen have the fewest amount as they are not known for scoring.

Next, let's take a look at a histogram of time on ice:

```{r timeOnIce histogram, echo=FALSE}

# histogram of timeOnIce
ggplot(data=data) +
  geom_histogram(aes(x=timeOnIce), binwidth=75, color="#010101", fill="#FFB81C") +
  labs(x="Time on Ice (sec)", y="Frequency") +
  ggtitle("Distribution of Time on Ice")

```



Finally, we will examine some statistics regarding the nationality of players.

```{r table of nationality, echo=FALSE}

table(data$nationality)

```

```{r nationality and shots bar chart, echo=FALSE}

# bar chart of nationality and shots taken
avg_shots_by_pos = data %>% 
  group_by(nationality) %>%
  summarise(avg_shots = mean(shots)) %>% ungroup

shots_by_pos <- ggplot() +
      geom_col(aes(x = nationality, y = avg_shots), data =
      avg_shots_by_pos, color = "#010101", fill = "#FFB81C")+
      labs(x="Nationality", y="Average Shots") +
      ggtitle("Shots per Game by Nationality")
  
shots_by_pos

```  

We can see from the table that some countries have many more players than others, such as the United States and Canada. This table coincides with the following bar chart, as countries with the fewest players will typically take the fewest amount of shots (Italy, Poland, and Finland), and vice versa (Canada, Slovakia, and USA). Germany (DEU) is an outlier from these visuals, as they have one of the fewest number of players (753), while taking the most shots per game on average (2.13).  


After completing our exploratory analysis, we made some final adjustment to the code before beginning our random forest analysis.  


```{r final data cleaning, echo=FALSE}

### FINAL CLEANING ----
#Coverting character variables to factors to use for forest
data <- subset(data, select = -c(goals))
data$firstName <- as.factor(data$firstName)
data$lastName <- as.factor(data$lastName)
data$nationality <- as.factor(data$nationality)
data$primaryPosition <- as.factor(data$primaryPosition)
data$shootsCatches <- as.factor(data$shootsCatches)
data$score <- as.factor(data$score)
str(data)

```

## Random Forest

To begin creating our random forest. We first split the data between training and testing sets. 70% of the data was used for training, while the remaining 30% was used for testing. These splits are described below:

```{r training and testing dataframe}

#-------- CREATE TRAINING AND TESTING SETS -----------
RNGkind(sample.kind = "default")
set.seed(3763)
train.idx <- sample(x=1:nrow(data), size=.7*nrow(data))
train.df <- data[train.idx, ]
test.df <- data[-train.idx, ]

train.df <- subset(train.df, select=-c(firstName, lastName)) #remove first name and last name from train.df

```

Now that we have established our training and testing data, we can fit an initial random forest.

```{r first forest}

#-------- FIT THE FOREST -----------
forest1 <- randomForest(score ~ .,
                        data=train.df, #TRAINING DATA
                        ntree = 1000, #B = the number of classification trees in forest
                        mtry = 4, #choose m: sqrt(18) = 4.25 approx
                        importance = T)

forest1 # base OOB error = 13.01%

```

```{r first forest plot}

plot(forest1)
```

To find the best forest, we tuned the number of explanatory variables used in each decision tree, also known as mtry. Since this dataset had 18 predictor variables, we ran a loop to build a random forest using anywhere from 1 to 18 variables.  

```{r forest tuning}

# ---- TUNE FOREST -----------
# tune mtry, the number of predictor variables used in the random forest
mtry <- c(1:18)

# empty data frame for m and oob error
keeps <- data.frame(m=rep(NA, length(mtry)),
                    OOB_error_rate = rep(NA, length(mtry)))

# loop through different values of mtry
for(idx in 1:length(mtry)){
  print(paste0("Fitting m = ", mtry[idx]))
  tempforest <- randomForest(score ~.,
                             data=train.df,
                             ntree=1000,
                             mtry = mtry[idx], #mtry is varying
                             na.action = na.roughfix) #impute missings! otherwise get error
  #record iteration's m value in j'th row
  keeps[idx, "m"] <- mtry[idx]   #record OOB error, corresponding mtry for each forest fit
  keeps[idx, "OOB_error_rate"] <- mean(predict(tempforest) != train.df$score)
}
keeps # best OOB error = 12.92375% at m=5

# plot the OOB error rates vs m
ggplot(data=keeps) +
  geom_line(aes(x=m, y=OOB_error_rate)) + 
  geom_point(aes(x=m, y=OOB_error_rate)) +
  labs(x="m (mtry) value", y="Out of Bag error rate (OOB)") +
  theme_bw() + 
  scale_x_continuous(limits=c(1,18), breaks=seq(1,18,1)) +
  ylim(0.125,0.150) +
  ggtitle("Tuned Random Forest Performance Based on the Number of Predictor Variables (m)")
  

# grab the best mtry value automatically
optimal_m <- which.min(keeps[,"OOB_error_rate"])

```

Typically, we found the OOB error rate to be minimized when mtry was 5 or 6. This was logical given that the generally accepted value for mtry is the square root of the number of explanatory variables in the dataset, which would theoretically be 4.24. Now that we know the optimal value for mtry, we fit a final forest.  

```{r final forest}

# fit final forest
forest2 <- randomForest(score ~.,
                        data=train.df, #TRAINING DATA
                        ntree = 1000,
                        mtry = optimal_m,
                        importance = T,
                        na.action = na.roughfix)
forest2 # OOB error = 12.88%

```

From our final forest we could plot an ROC curve to pull other important metrics such as pi*, specificity, and sensitivity.  

```{r forest ROC curve}

# ---- PLOT ROC CURVE ---------------
# establish p-hat ... "Yes" is a positive event
pi_hat <- predict(forest2, test.df, type="prob")[,"Yes"]
# create curve
rocCurve <- roc(response = test.df$score, #supply truth (from test set)
                predictor = pi_hat, #supply predicted PROBABILITIES of positive case
                levels = c("No", "Yes")) #(negative, positive)
# plot basic ROC curve
plot(rocCurve, print.thres = TRUE, print.auc = TRUE)


# create a column of predicted values in the test data
pi_star <- coords(rocCurve, "best", ret="threshold")$threshold[1]
test.df$score_pred <- as.factor(ifelse(pi_hat > pi_star, "Yes", "No"))
#View(test.df)

```

We recognize that due to randomness the following values could be slightly different between the visual and the written remarks.

From the visual we see that the AUC is  roughly 0.817. The pi* is 0.144, therefore we will only predict a goal when the probability of scoring is greater than 0.144.

We found the specificity to be 0.704, which means that when a goal is not scored, we correctly predict that a player does not score 70.4% of the time.

The sensitivity is 0.761, meaning that when a goal is scored, we correctly predict that a player will score 76.1% of the time.

Now that we have performed a complete analysis utilizing the random forest, we will transition to using a GLM for greater interpretability.

## GLM

To begin, we need to create a variable importance plot to determine which predictor variables provide the most value. This was initially calculated when compiling our final forest.

```{r Variable Importance Plot, include=FALSE}

# -------- FITTING A GLM -----
# make a variable importance plot
vi <- as.data.frame(varImpPlot(forest2, type = 1))
vi$Variable <- rownames(vi)

```


```{r Variable Importance Plot Visual}

ggplot(data = vi) +
  geom_bar(aes(x = reorder(Variable,MeanDecreaseAccuracy), 
              weight = MeanDecreaseAccuracy), 
              position ="identity", color = "#010101", fill="#FFB81C") + 
  coord_flip() + 
  labs(x = "Variable Name", y = "Importance") +
  ggtitle("Variable Importance Plot for Predicting 'score'")  

```

From the variable importance plot we see that `shots` and `timeOnIce` were our important variables.  

Using this plot we can then perform a forward stepwise regression to find the best model by AIC and BIC criterion. The GLM that minimizes each of these values will signify the best fit. Each of these models will use a Bernoulli random component with a logit link given the binary nature of our response variable. This final model can then be used to reliably predict the probability of a player scoring one or more goals in their next game.  

```{r GLM1a, results='hide'}

### USING VARIABLE IMPORTANCE PLOT
# add variables to find optimum model (forward stepwise regression)
# fit logistic regression
glm1a = glm(score ~ shots + timeOnIce, 
           data=data, family = binomial(link = "logit"))
AIC(glm1a) # 23580.5
BIC(glm1a) # 23605.7

```

```{r GLM1b, results='hide'}

glm1b = glm(score ~ shots + timeOnIce + primaryPosition, 
           data=data, family = binomial(link = "logit"))
AIC(glm1b) # 22894.29
BIC(glm1b) # 22944.67
# best model according to BIC

```

```{r GLM1c, results='hide'}

glm1c = glm(score ~ shots + timeOnIce + primaryPosition + weight, 
           data=data, family = binomial(link = "logit"))
AIC(glm1c) # 22891.6
BIC(glm1c) # 22950.38 ... BIC increasing again

```

```{r GLM1d, results='hide'}

glm1d = glm(score ~ shots + timeOnIce + primaryPosition + weight +
             nationality, 
           data=data, family = binomial(link = "logit"))
AIC(glm1d) # 22865.45

```

```{r GLM1E, results='hide'}

glm1e = glm(score ~ shots + timeOnIce + primaryPosition + weight +
             nationality + hits, 
           data=data, family = binomial(link = "logit"))
AIC(glm1e) # 22853.99

```

```{r GLM1F, results='hide'}

glm1f = glm(score ~ shots + timeOnIce + primaryPosition + weight + 
             nationality + hits + age, 
           data=data, family = binomial(link = "logit"))
AIC(glm1f) # 22829.79

```

```{r GLM1G, results='hide'}

glm1g = glm(score ~ shots + timeOnIce + primaryPosition + weight + 
             nationality + hits + age + faceoffTaken, 
           data=data, family = binomial(link = "logit"))
AIC(glm1g) # 22827.8
# best model according to AIC, we will use this as the final model

```

```{r GLM1H, results='hide'}

glm1h = glm(score ~ shots + timeOnIce + primaryPosition + weight + 
              nationality + hits + age + faceoffTaken + giveaways, 
            data=data, family = binomial(link = "logit"))
AIC(glm1h) # 22829.38 ... AIC increasing again

```

### Final GLM Model:
```{r FINAL GLM}

# clean up model name
glm1 = glm1g
summary(glm1)

```

To ensure the accuracy of our process we also performed a backwards regression:

```{r backwards regression}

### USING BACKWARDS REGRESSION
# fully saturated model
glm2 = glm(score ~ shots + timeOnIce + primaryPosition + weight + nationality +
             hits + age + faceoffTaken + giveaways + blocked + height_cm + takeaways +
             faceOffWins + assists + penaltyMinutes + shootsCatches + powerPlayAssists +
             shortHandedAssists,
           data = data, family = binomial(link = "logit"))

# use stepAIC backwards regression to find the best model
stepAIC(glm2, direction = 'backward', trace=FALSE)

# model specified by stepAIC
glm2 = glm(score ~ shots + timeOnIce + primaryPosition + weight + nationality +
           hits + age + height_cm + faceOffWins + powerPlayAssists,
           data=data, family = binomial(link="logit"))
summary(glm2)
AIC(glm2) # 22820.26


```
Although this model has the lowest AIC, the stepAIC function may not lead to the best combination of predictors as it lends itself to overfitting and inconsistent results. Therefore, we select GLM1 as the final Bernoulli model. 

Not only is this a more parsimonious model than GLM2, it was also built using a random forest algorithm, which is guaranteed to not overfit and provide the most reliable results.  


Thus we define our final model as the following:

$$\text{Random Component}: \text{score}_i \sim Bernoulli (\pi_i)$$

$\text{Systematic Component}:$
$$\text{Link Function}: g(\pi_i) = \eta_i = \log\left( \dfrac{\pi_i}{1-\pi_i}  \right)$$
$$\begin{align} \text{Linear Predictor}:\eta_i = \beta_0 &+ \beta_1(\text{shots}) + \beta_2(\text{timeOnIce}) + \beta_3(\text{primaryPositionD}) \\ &+ \beta_4(\text{primaryPositionLW}) + \beta_5(\text{primaryPositionRW}) + \beta_6(\text{weight}) \\ &+ \beta_7(\text{nationalityCZE}) + \beta_8(\text{nationalityDEU}) + \beta_9(\text{nationalityFIN}) \\ &+ \beta_{10}(\text{nationalityITA}) + \beta_{11}(\text{nationalityLVA}) + \beta_{12}(\text{nationalityPOL}) \\ &+ \beta_{13}(\text{nationalityRUS}) + \beta_{14}(\text{nationalitySVK}) +\beta_{15}(\text{nationalitySWE}) \\ &+\beta_{16}(\text{nationalityUSA}) + \beta_{17}(\text{hits}) +\beta_{18}(\text{age}) + \beta_{19}(\text{faceoffTaken}) \end{align}$$
```{r FINAL GLM coefficients}
# coefficients of each variable
betas <- coef(glm1)
betas
```

$$\begin{align} \eta_i = -2.723 &+ 0.4823(\text{shots}) + 0.0006238(\text{timeOnIce}) - 0.9205(\text{primaryPositionD}) \\ &+ 0.2633(\text{primaryPositionLW}) + 0.3246(\text{primaryPositionRW}) + 0.00007159(\text{weight}) \\ &+ 0.001016(\text{nationalityCZE}) + 0.02479(\text{nationalityDEU}) - 0.2233(\text{nationalityFIN}) \\ &- 0.1091(\text{nationalityITA}) - 0.1176(\text{nationalityLVA}) - 0.1332(\text{nationalityPOL}) \\ &+ 0.1793(\text{nationalityRUS}) + 0.1566(\text{nationalitySVK}) - 0.1783(\text{nationalitySWE}) \\ &- 0.2402(\text{nationalityUSA}) - 0.06965(\text{hits}) - 0.01478(\text{age}) + 0.006905(\text{faceoffTaken}) \end{align}$$
In our final model, Centers for `primaryPosition` and Canada (CAN) for `nationality` were our baseline reference variables.   
```{r FINAL GLM exponentiated variables}
# exponentiated coefficients for interpretations
exp(betas)
```  

Looking at the exponentiated coefficients we can make a few interpretations:

Exponentiated coefficient for `shots`: 1.619792

- Holding all other factors constant, the odds of scoring increase by a factor of 1.619792 for every additional shot taken, or roughly 62%. 

Exponentiated coefficient for `nationalityUSA`: 0.7864956

- Holding all other factors constant, the odds of a U.S. player scoring are 0.7864956 times the odds of a Canadien player scoring, or roughly 79%.

coefficient for `primaryPositionRW`: 0.3245554

coefficient for `primaryPositionD`: -0.9205181

Holding all other factors constant, the odds that a right wing scores are $e^{0.3245554-(-0.9205181)}=3.473$ times the odds of a defensemen scoring. Thus, the odds of a right wing scoring are almost 350% larger than the odds of a defensemen.


```{r FINAL GLM confidence intervals, warning=FALSE}
# 95% confidence intervals of each variable
confint <- confint(glm1)
exp(confint)
```
  
Exponentiated 95% confidence interval for `shots`: 1.5853 - 1.6553
Holding all other factors constant, we are 95% confident that the true multiplicative change in odds of a player scoring a goal for each additional shot are in the range (1.5853, 1.6553). In other words, for each additional shot the player takes, we expect there to a 58.53-65.53% increase in the odds that they score a goal.

Exponentiated 95% confidence interval for `nationalityUSA`: 0.71659 - 0.86282
Holding all other factors constant, we are 95% confident that the true multiplicative change in odds of a United States player scoring a goal compared to that of a Canadian player are in the range (0.71659, 0.86282). In other words, the odds of an American player scoring are roughly 71.659-86.282% of that of a Canadian player.

## Conclusion

### Predictions

Now we can create some predictions based off our final GLM.

For example we can perform a prediction on David Pastrnak, who has been a great player for Boston this season. The following are some variables from a game against the Arizona Coyotes that are relevant to this model:

| Variable | Value |
| :---: | :---: |
| Shots | 9 |
| timeOnIce | 1,336 sec |
| PrimaryPosition | RW |
| Weight | 195 |
| Nationality | Czech |
| Hits |  0 |
| Age | 26 |
| FaceoffTaken | 0 |

Inputting these into the model we get:

$$\begin{align} \log \left( \dfrac{\pi_i}{1-\pi_i} \right) = -2.723 &+ 0.4823(9) + 0.0006238(1,336) - 0.9205(0) \\ &+ 0.2633(0) + 0.3246(1) + 0.00007159(195) \\ &+ 0.001016(1) + 0.02479(0) - 0.2233(0) - 0.1091(0) \\ &- 0.1176(0) - 0.1332(0) + 0.1793(0) + 0.1566(0) \\ &- 0.1783(0) - 0.2402(0) - 0.06965(0) - 0.01478(26) \\ &+ 0.006905(0) \end{align}$$

Thus we get: 

$$\dfrac{\pi_i}{1-\pi_i} = e^{2.40639285} = 11.09$$

This means that the odds David Pastrnak scores a goal in the game are 11.09. The probability that Pastrnak scores is:

$$\dfrac{e^{2.40639285}}{1+e^{2.40639285}} = 91.73 \%$$

Since one variable that a Coach can control is `timeOnIce`, what would be the amount of time that David Pastrnak needs to play to have at least a 75% chance of scoring a goal? Given some preset values, we can determine the following:

| Variable | Value |
| :---: | :---: |
| Shots | 6 shots (average per game) |
| timeOnIce | ? |
| PrimaryPosition | RW |
| Weight | 195 |
| Nationality | Czech |
| Hits |  1 |
| Age | 26 |
| FaceoffTaken | 1 (may take 1 or 2 faceoffs a game) |

$$\begin{align} \log \left( \dfrac{0.75}{1-0.75} \right) < -2.723 &+ 0.4823(6) + 0.0006238(\text{timeOnIce}) - 0.9205(0) \\ &+ 0.2633(0) + 0.3246(1) + 0.00007159(195) \\ &+ 0.001016(1) + 0.02479(0) - 0.2233(0) - 0.1091(0) \\ &- 0.1176(0) - 0.1332(0) + 0.1793(0) + 0.1566(0) \\ &- 0.1783(0) - 0.2402(0) - 0.06965(0) - 0.01478(26) \\ &+ 0.006905(1) \end{align}$$
$$\begin{align} 1.098612289 &< 0.0006238(\text{timeOnIce}) +0.13300105 \\ \text{timeOnIce} &> 1547.95 \text{ secs} = 25 \text{ minutes and } 48 \text{ seconds} \end{align}$$

Thus, Pastrnak would need to be playing at least 25 to 26 minutes to have a 75% probability of scoring. Given that a regulation NHL hockey game consists of three 20-minute periods, he would need to be playing for nearly half of the game.  

### Visualizations

Now that we have identified our final model and the most valuable variables, we can perform some post-analysis visualizations:  

```{r shots and time on ice, echo=FALSE}

# colored scatter plot of top 2 variables - shots and timeOnIce
  # timeOnIce x-axis, shots y-axis, points colored by Score
ggplot(data = data) +
  geom_point(aes(x = timeOnIce, y = shots, color = primaryPosition)) +
  geom_jitter(aes(x = timeOnIce, y= shots, color = primaryPosition), alpha = I(0.7)) +
  labs(x = "Time On Ice", y = "Shots")+
  ggtitle("Shots by Position Player Time On Ice") +
  scale_color_manual(values=c("#744F28", "#FFB81C", "#DDCBa4","#010101"), "Position")


```

```{r hist of score by age, echo=FALSE}

# histogram of score by age
ggplot(data = data) +
  geom_bar(aes(x = age, fill = score), position = "identity") +
  labs(x = "Age", y = "Frequency")+
  ggtitle("Goals scored by Age") +
  scale_fill_manual(values=c("#744F28", "#FFB81C"), "Goals \nScored")

```

```{r position and faceoffs, echo=FALSE}

# bar chart of primaryPosition and faceoffTaken
options(scipen = 999)
ggplot(data=data) +
  geom_col(aes(x=primaryPosition, y=faceoffTaken), fill="#744F28") +
  labs(x="Position", y="Number of Face Offs Taken") + 
  ggtitle("Total Face Offs Taken by Position") +
  scale_y_continuous(limits=c(0,100000), breaks=seq(0,100000,20000))

```



```{r nationality and scored hist, echo=FALSE}

# histogram of nationality and goals scored
ggplot(data=data) +
  geom_bar(aes(x=nationality, fill=score), position="fill") +
  labs(x="Nationality", y="Proportion") +
  ggtitle("Proportion of Goals Scored by Nationality") +
  scale_fill_manual(values=c("#744F28", "#FFB81C"), "Goals \nScored")

```



# Final Remarks  

Based on our final model and comprehensive visuals there are several important takeaways that could benefit the Boston Bruin's coaching staff. From our general linear model (GLM) we saw that the number of shots taken and time on ice are two important predictors for determining whether a player scores. Thus, we recommend coaches give their best players more opportunities to take shots and play them more frequently. Furthermore, from the GLM we saw that in the past, certain positions such as forwards tended to score more. Additionally, nationalities like Slovakia and Russia lead to a higher probability of scoring. It is also interesting to note that hits decreases the likelihood of goals due to higher hitting players usually not being good scorers.  

From the variable importance plot we could see that takeaways (players stealing the puck from opponents) and the handedness of a player did not matter. Initially, we suspected that these variables would have been more important predictors.   

In our visual plotting shots against time and coloring by position, there were many defensemen who had a lot of time on ice along with a few forwards. Furthermore, for those taking a lot of shots, it was almost all forwards. This aligns with what we interpreted from the GLM. When looking at goals scored by age, we saw a larger proportion were scored by younger ages. This makes sense as younger players are typically more athletic and better goal scorers. 

Finally, for the client's awareness, we wanted to display the number of face offs taken by position. Centers clearly take the majority of face offs for the team, which is logical given the position's primary responsibilities. The only time a center would not participate in a face off is if they were in the penalty box, in which a wingman would replace their spot. Finally, we showed the proportion of goals scored by nationality. Most notably, Czech and Russian players had a higher proportion of goals scored compared to the United States or Canada who make up the majority of the team.  

Future exploration could involve looking at interaction terms, and trying to quantify team chemistry as goal-scorers could either help or hinder the team's performance. Furthermore, quantifying the opposing team to get more specific game-by-game results could improve the accuracy of this analysis.